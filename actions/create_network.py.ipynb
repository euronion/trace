{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1279ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import re\n",
    "import types\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pypsa\n",
    "from _helpers import (calculate_annual_investment, calculate_annuity,\n",
    "                      configure_logging, extract_technology, get_bus_unit,\n",
    "                      read_efficiencies)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _do_units_match(unit1, unit2):\n",
    "    \"\"\"Rough checker if two units match.\n",
    "\n",
    "    Catches mismatches between e.g. t, m^3 and W.\n",
    "    Does not catch order of magnitude mismatches by SI prefixes.\"\"\"\n",
    "\n",
    "    def _stripper(u):\n",
    "        import re\n",
    "\n",
    "        # remove optional Si prefixes 'M', 'k' and per hour ('/h', 'h') indicators\n",
    "        u = re.match(\"[Mk]?(.+?)\\/?h?$\", re.split(\"_|-\", u)[0]).groups()[0]\n",
    "\n",
    "        # Specific indicators to remove, a bit hacky and may cause problems they do not match. Hotfix.\n",
    "        for p in [\"CO2\", \"/km\"]:\n",
    "            u = u.replace(p, \"\")\n",
    "\n",
    "        return u\n",
    "\n",
    "    return _stripper(unit1) == _stripper(unit2)\n",
    "\n",
    "\n",
    "def create_network():\n",
    "    \"\"\"Create the pypsa network scaffolding for the ESC.\"\"\"\n",
    "\n",
    "    # Modify PyPSA 'Link' component to allow for 2 output busses by overwriting component_attrs\n",
    "    # c.f. https://www.pypsa.org/examples/chp-fixed-heat-power-ratio.html\n",
    "\n",
    "    # Load additional components\n",
    "    with open(snakemake.input[\"additional_components\"], \"rb\") as f:\n",
    "        override_component_attrs = pickle.load(f)\n",
    "\n",
    "    # Create network with modified link-component\n",
    "    network = pypsa.Network(override_component_attrs=override_component_attrs)\n",
    "\n",
    "    # Load network components from csv files\n",
    "    network.import_from_csv_folder(snakemake.input[\"network\"])\n",
    "\n",
    "    # Equally weighted snapshots, year defined via config\n",
    "    year = int(snakemake.params[\"era_year\"])\n",
    "    snapshots = pd.date_range(str(year), str(year + 1), freq=\"H\", inclusive=\"left\")\n",
    "    network.set_snapshots(snapshots)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def attach_efficiencies(network):\n",
    "    \"\"\"Attach dedicated efficiencies from file.\n",
    "\n",
    "    The efficiencies are from an additional csv file and added to the links in the pypsa network\n",
    "    Format for efficiencies.csv file:\n",
    "    * \"from\" and \"to\" must substrings of the bus names\n",
    "    * \"process\" must be a substring of the name of the link\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    network : pypsa.network\n",
    "        network with external efficiencies attached to all links.\n",
    "\n",
    "    \"\"\"\n",
    "    efficiencies = read_efficiencies(\n",
    "        snakemake.input[\"efficiencies\"], snakemake.wildcards[\"year\"]\n",
    "    )\n",
    "\n",
    "    def get_efficiency(tech, src_bus, tar_bus):\n",
    "\n",
    "        tech = extract_technology(tech)\n",
    "        src = extract_technology(src_bus)\n",
    "        tar = extract_technology(tar_bus)\n",
    "\n",
    "        efficiency = efficiencies[\n",
    "            (efficiencies[\"process\"] == tech)\n",
    "            & (efficiencies[\"to\"] == tar)\n",
    "            & (efficiencies[\"from\"] == src)\n",
    "        ]\n",
    "\n",
    "        if efficiency.empty is True:\n",
    "            return np.nan\n",
    "\n",
    "        # Check if all units match\n",
    "        src_unit = get_bus_unit(src_bus, network)\n",
    "        tar_unit = get_bus_unit(tar_bus, network)\n",
    "        unit_mismatch = None\n",
    "        if (efficiency[[\"from_unit\", \"to_unit\"]] == \"p.u.\").any(\n",
    "            axis=None\n",
    "        ) == True:  #'==' b/c pd.any returns np.bool\n",
    "\n",
    "            if (efficiency[[\"from_unit\", \"to_unit\"]] == \"p.u.\").all(\n",
    "                axis=None\n",
    "            ) == False:  #'==' b/c pd.any returns np.bool\n",
    "                unit_mismatch = \"One efficiency in [p.u.], but the other one not.\"\n",
    "            elif _do_units_match(src_unit, tar_unit) is False:\n",
    "                unit_mismatch = f\"Unit of bus {src_bus} [{src_unit}] does not match {tar_bus} [{tar_unit}].\"\n",
    "\n",
    "        elif _do_units_match(src_unit, efficiency[\"from_unit\"].item()) is False:\n",
    "            unit_mismatch = (\n",
    "                f\"Source bus unit {src_bus} [{src_unit}] does not match unit \"\n",
    "                f'in registered efficiencies.csv [{efficiency[\"from_unit\"].item()}].'\n",
    "            )\n",
    "        elif _do_units_match(tar_unit, efficiency[\"to_unit\"].item()) is False:\n",
    "            unit_mismatch = (\n",
    "                f\"Target bus unit {tar_bus} [{tar_unit}] does not match unit \"\n",
    "                f'in registered efficiencies.csv [{efficiency[\"to_unit\"].item()}].'\n",
    "            )\n",
    "\n",
    "        if unit_mismatch:\n",
    "            raise ValueError(f\"Mismatching units for {tech}: {unit_mismatch}.\")\n",
    "\n",
    "        return efficiency[\"efficiency\"].item()\n",
    "\n",
    "    links = network.links\n",
    "\n",
    "    for idx, row in links.iterrows():\n",
    "\n",
    "        lead_efficiency = get_efficiency(\n",
    "            extract_technology(row.name), row[\"bus0\"], row[\"bus1\"]\n",
    "        )\n",
    "        if np.isnan(lead_efficiency):\n",
    "            logger.error(\n",
    "                f\"No efficiency found for link {row.name} between \"\n",
    "                f\"{row['bus0']} and {row['bus1']}.\"\n",
    "            )\n",
    "        else:\n",
    "            links.loc[idx, \"efficiency\"] = lead_efficiency\n",
    "\n",
    "        additional_buses = {\n",
    "            c for c in links.columns if c.startswith(\"bus\") and row[c] != \"\"\n",
    "        } - {\"bus0\", \"bus1\"}\n",
    "        for b in additional_buses:\n",
    "\n",
    "            # by design decision all buses busn (n>1, e.g. bus2, bus3, ...) either:\n",
    "            # case 1. contribute to the output to bus1, e.g. bus2 feeds into bus1\n",
    "            # or\n",
    "            # case 2. are few by bus0.\n",
    "            # Try to retrieve both efficiencies: One should always be nan, the other one is taken.\n",
    "            # If either one are nan or not nan, throw an error.\n",
    "            #\n",
    "            # Case 1.:\n",
    "            # Efficiencies are provided for the conversion from bus2 to bus1\n",
    "            # and are thus weighted by the primary efficiency of bus1\n",
    "            # Efficiencies are have to become negative to correctly account for the flow.\n",
    "            follow_efficiency = (\n",
    "                (-1)\n",
    "                * lead_efficiency\n",
    "                / get_efficiency(extract_technology(row.name), row[b], row[\"bus1\"])\n",
    "            )\n",
    "\n",
    "            # Case 2.:\n",
    "            # Efficiencies are provided for conversion from bus0 to busn.\n",
    "            # This type of efficiency does not need to be adjusted.\n",
    "            regular_efficiency = efficiency = get_efficiency(\n",
    "                extract_technology(row.name), row[\"bus0\"], row[b]\n",
    "            )\n",
    "\n",
    "            e = np.array([regular_efficiency, follow_efficiency])\n",
    "\n",
    "            if np.isnan(e).all():\n",
    "                logger.error(\n",
    "                    f\"No efficiency found for link {row.name} between \"\n",
    "                    f\"{row[b]} <-> ({row['bus0']} or {row['bus1']}).\"\n",
    "                )\n",
    "            elif np.isnan(e).sum() == 0:\n",
    "                logger.error(\n",
    "                    f\"Two efficiencies found for link {row.name} between \"\n",
    "                    f\"{row[b]} <-> ({row['bus0']} and {row['bus1']}). \"\n",
    "                    f\"Efficiency must by unambigious.\"\n",
    "                )\n",
    "            else:\n",
    "                links.loc[idx, \"efficiency\" + b.replace(\"bus\", \"\")] = e[~np.isnan(e)][0]\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def override_costs_for_special_cases(n):\n",
    "\n",
    "    # battery inverter represented by two links (charging and discharging),\n",
    "    # while costs in cost data are for bidirectional inverter --> correction here\n",
    "    links = n.links\n",
    "    idx = links.filter(like=\"battery inverter\", axis=0).index\n",
    "    links.loc[idx, \"capital_cost\"] /= 2.0\n",
    "\n",
    "    return n\n",
    "\n",
    "\n",
    "def attach_costs(network):\n",
    "    \"\"\"\n",
    "    Attach the overnight investment costs (capital costs) to the network.\n",
    "\n",
    "    Costs are calculated from investment costs and FOM using EAC method\n",
    "    and wacc as specified via config/snakemake.input files.\n",
    "    Components name need to follow the scheme '<name> (<exp|imp>)'\n",
    "    where '<name>' must correspond to the component in the costs.csv file.\n",
    "\n",
    "    Requires efficiencies of the network to be already attached.\n",
    "    \"\"\"\n",
    "    wacc = pd.read_csv(\n",
    "        snakemake.input[\"wacc\"], comment=\"#\", index_col=\"region\", keep_default_na=False\n",
    "    )\n",
    "    wacc = wacc.loc[snakemake.wildcards[\"from\"], scenario[\"wacc\"]]\n",
    "    wacc *= scenario[\"modifiers\"][\"wacc\"]  # Apply scenario modifier\n",
    "\n",
    "    costs = pd.read_csv(snakemake.input[\"costs\"], index_col=[\"technology\", \"parameter\"])\n",
    "\n",
    "    def attach_component_costs(network, component):\n",
    "\n",
    "        components = getattr(network, component)\n",
    "        for idx, row in components.iterrows():\n",
    "\n",
    "            try:\n",
    "                tech = costs.loc[extract_technology(idx)]\n",
    "            except KeyError:\n",
    "                logger.warning(f\"No cost assumptions found for {idx}.\")\n",
    "                continue\n",
    "\n",
    "            if \"commodity\" in tech.index:\n",
    "                logger.info(f\"Adding commodity costs for {tech.index}\")\n",
    "                assert _do_units_match(\n",
    "                    get_bus_unit(row[\"bus\"], network),\n",
    "                    tech.loc[\"commodity\"][\"unit\"].replace(\n",
    "                        \"EUR/\", \"\"\n",
    "                    ),  # Assume unit for fuel/commodities is EUR/<bus unit>\n",
    "                ), \"Mismatching units\"\n",
    "                components.loc[idx, \"marginal_cost\"] = tech.loc[\"commodity\"][\"value\"]\n",
    "                continue\n",
    "\n",
    "            # Compare units between bus and cost data; scale investment on-demand\n",
    "            investment_unit = (\n",
    "                tech.loc[\"investment\"][\"unit\"]\n",
    "                .replace(\"EUR/\", \"\")\n",
    "                .replace(\"(\", \"\")\n",
    "                .replace(\")\", \"\")\n",
    "            )\n",
    "\n",
    "            investment_factor = 1.0\n",
    "\n",
    "            # Determine unit of the bus - depends on component type\n",
    "            # stores -> attached to one bus, situation clear\n",
    "            # generators -> attached to one bus, situation clear\n",
    "            # links -> attached to >=2 buses, use additional column in .csv to determine the bus to scale to\n",
    "            bus_unit = bus0_unit = bus1_unit = None\n",
    "            if component == \"stores\" or component == \"generators\":\n",
    "                bus_unit = network.buses.loc[row[\"bus\"]][\"unit\"]\n",
    "            elif component == \"links\":\n",
    "\n",
    "                if row[\"scale_costs_based_on\"] not in [\"bus0\", \"bus1\"]:\n",
    "                    raise NotImplementedError(\n",
    "                        f\"Scaling of costs for link {idx} to others than bus0 or bus1 not implemented.\"\n",
    "                    )\n",
    "\n",
    "                bus_unit = network.buses.loc[row[row[\"scale_costs_based_on\"]]][\"unit\"]\n",
    "                if row[\"scale_costs_based_on\"] == \"bus1\":\n",
    "                    # bus1 is output by convention; cost in output units, i.e. scale to input units\n",
    "                    investment_factor *= row[\"efficiency\"]\n",
    "\n",
    "            # Consistency check: Correct units (ignore first letter for prefixed values)\n",
    "            # Warning: Does not catch case were bus unit is only a single letter\n",
    "            if _do_units_match(investment_unit, bus_unit) is False:\n",
    "                raise ValueError(\n",
    "                    f'Could not find matching cost data for {component} \"{idx}\": '\n",
    "                    f\"Expected {bus_unit} based on network, but found {investment_unit} in cost data.\"\n",
    "                )\n",
    "\n",
    "            prefix_bus_unit = bus_unit[0]\n",
    "            prefix_investment_unit = investment_unit[0]\n",
    "\n",
    "            if prefix_bus_unit == prefix_investment_unit:\n",
    "                investment_factor *= 1.0\n",
    "            elif prefix_bus_unit == \"M\" and prefix_investment_unit == \"k\":\n",
    "                investment_factor *= 1.0e3\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Cannot scale between {prefix_bus_unit} and {prefix_investment_unit} \"\n",
    "                    f\"for {idx} costs.\"\n",
    "                )\n",
    "\n",
    "            # Some technologies are without FOM values\n",
    "            # (e.g. battery capacity where FOM is attributed to the link/inverter/charger capacities)\n",
    "            try:\n",
    "                fom = tech.loc[\"FOM\", \"value\"]\n",
    "            except KeyError:\n",
    "                logger.info(f\"No FOM for {idx}, assuming 0%.\")\n",
    "                fom = 0.0\n",
    "\n",
    "            logger.info(\n",
    "                f\"Rescaling investment for {idx} by {investment_factor} to match units and efficiencies.\"\n",
    "            )\n",
    "            capital_cost = calculate_annuity(\n",
    "                tech.loc[\"investment\", \"value\"] * investment_factor,\n",
    "                fom,\n",
    "                tech.loc[\"lifetime\", \"value\"],\n",
    "                wacc,\n",
    "            )\n",
    "            components.loc[idx, \"capital_cost\"] = capital_cost\n",
    "\n",
    "        return network\n",
    "\n",
    "    network = attach_component_costs(network, \"links\")\n",
    "    network = attach_component_costs(network, \"stores\")\n",
    "    network = attach_component_costs(network, \"generators\")\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def scale_transportation_with_distance(\n",
    "    n, link_types=[\"HVDC overhead\", \"HVDC submarine\", \"pipeline\", \"submarine pipeline\"]\n",
    "):\n",
    "\n",
    "    \"\"\"Scales the cost and efficiency of specific links (transport options) by distance.\n",
    "\n",
    "    Only implemented for:\n",
    "     * HVDC overhead|submarine\n",
    "     * H2 (g) [submarine] pipeline\n",
    "     * CH4 (g) [submarine] pipeline\n",
    "    Names of the components must match the regex, otherwise no scaling happens.\n",
    "    Check the 'length' attribute of a link to see if it has been scaled and to which distance.\n",
    "    A distance of 0 is forcefully set to distance=1 for compressors to keep this component\n",
    "    and conversion in the supply chain including its connection to the electricity bus.\n",
    "    Costs and efficiencies are - if affected - scaled accordingly.\n",
    "    Submarine distances are substracted from HVDC overhead and onland pipelines from their distance.\n",
    "    \"\"\"\n",
    "\n",
    "    links = n.links\n",
    "    distances = pd.read_csv(\n",
    "        snakemake.input[\"distances\"], comment=\"#\", quotechar='\"', keep_default_na=False\n",
    "    )\n",
    "\n",
    "    # These are the components to scale\n",
    "    # they may behave differently, case-by-case decision below\n",
    "    mapping = {\n",
    "        \"HVDC overhead\": {\n",
    "            \"name_pattern\": \"HVDC overhead$\",  # pattern to select components from n.links using regex\n",
    "            \"distance_type\": \"as-the-crow-flies\",  # entry as in snakemake.input['distances']\n",
    "            \"reduce_by_distance_type\": \"as-the-hake-swims\",  # entry as in snakemake.input['distances']\n",
    "            \"detour_factor_key\": \"transmission_line\",  # from snakemake.config\n",
    "        },\n",
    "        \"HVDC submarine\": {\n",
    "            \"name_pattern\": \"HVDC submarine$\",\n",
    "            \"distance_type\": \"as-the-hake-swims\",\n",
    "            \"detour_factor_key\": \"transmission_line\",\n",
    "        },\n",
    "        \"pipeline\": {\n",
    "            \"name_pattern\": \"(H2|CH4) \\(g\\) pipeline$\",\n",
    "            \"distance_type\": \"as-the-crow-flies\",\n",
    "            \"reduce_by_distance_type\": \"as-the-hake-swims\",\n",
    "            \"detour_factor_key\": \"pipeline\",\n",
    "        },\n",
    "        \"submarine pipeline\": {\n",
    "            \"name_pattern\": \"(H2|CH4) \\(g\\) submarine pipeline$\",\n",
    "            \"distance_type\": \"as-the-hake-swims\",\n",
    "            \"detour_factor_key\": \"pipeline\",\n",
    "        },\n",
    "        \"pipeline compressor\": {\n",
    "            \"name_pattern\": \"^(H2|CH4) \\(g\\) pipeline compressor(\\s\\(exp|imp\\))?\",\n",
    "            \"distance_type\": \"as-the-crow-flies\",\n",
    "            \"detour_factor_key\": \"pipeline\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    distances = distances[\n",
    "        (distances[\"region_a\"] == snakemake.wildcards[\"from\"])\n",
    "        & (distances[\"region_b\"] == snakemake.wildcards[\"to\"])\n",
    "    ].set_index(\"type\")[\"value\"]\n",
    "\n",
    "    efs = [c for c in links.columns if c.startswith(\"efficiency\")]\n",
    "\n",
    "    for link_type in link_types:\n",
    "\n",
    "        # Relevant properties for this link_type\n",
    "        m = mapping[link_type]\n",
    "\n",
    "        # All links in the network matching the link_type\n",
    "        idx = links.filter(regex=m[\"name_pattern\"], axis=0).index\n",
    "        if idx.empty is True:\n",
    "            continue  # Nothing to do\n",
    "\n",
    "        distance = distances[m[\"distance_type\"]]\n",
    "        if m.get(\"reduce_by_distance_type\", False):\n",
    "            distance -= distances[m[\"reduce_by_distance_type\"]]\n",
    "        distance *= snakemake.config[\"detour_factors\"][m[\"detour_factor_key\"]]\n",
    "\n",
    "        # For bookkeeping\n",
    "        links.loc[idx, \"length\"] = distance\n",
    "\n",
    "        # capital_cost in EUR/km, scale linearly\n",
    "        links.loc[idx, \"capital_cost\"] *= distance\n",
    "\n",
    "        # efficiencies which scale by power law per 1000km\n",
    "        links.loc[idx, \"efficiency\"] = (links.loc[idx, \"efficiency\"]) ** (\n",
    "            distance / 1.0e3\n",
    "        )\n",
    "\n",
    "        if links.loc[idx, efs].notnull().to_numpy().sum() != 1:\n",
    "            raise NotImplementedError(\n",
    "                f\"Scaling of link {idx} failed. \"\n",
    "                f\"Expected exactly one efficiency, but found more or less.\"\n",
    "            )\n",
    "\n",
    "    return n\n",
    "\n",
    "\n",
    "def add_shipping(n):\n",
    "    \"\"\"Adds optional shipping routes to the network.\n",
    "\n",
    "    Checks whether file \"<ESC>/ships.csv\" exists and - if it does - constructs\n",
    "    a shipping route with multiple convoys (as optimisation options) for this route\n",
    "    using standard PyPSA components.\n",
    "\n",
    "    Limitation: ONLY SUPPORTS ONE SHIPPING CONNECTION PER NETWORK AT THE MOMENT!\n",
    "    \"\"\"\n",
    "\n",
    "    fn = Path(snakemake.input[\"network\"]) / \"ships.csv\"\n",
    "\n",
    "    # Network without shipping routes\n",
    "    if not fn.exists():\n",
    "        return n\n",
    "    else:\n",
    "        ships = pd.read_csv(fn, comment=\"#\", index_col=\"name\")\n",
    "\n",
    "    props = pd.read_csv(\n",
    "        snakemake.input[\"shipping_properties\"],\n",
    "        comment=\"#\",\n",
    "        index_col=[\"name\", \"variable\"],\n",
    "    )\n",
    "\n",
    "    distances = pd.read_csv(\n",
    "        snakemake.input[\"distances\"], comment=\"#\", quotechar='\"', keep_default_na=False\n",
    "    )\n",
    "\n",
    "    wacc = pd.read_csv(\n",
    "        snakemake.input[\"wacc\"], comment=\"#\", index_col=\"region\", keep_default_na=False\n",
    "    )\n",
    "    wacc = wacc.loc[snakemake.wildcards[\"from\"], scenario[\"wacc\"]]\n",
    "    wacc *= scenario[\"modifiers\"][\"wacc\"]  # Apply scenario modifier\n",
    "\n",
    "    costs = pd.read_csv(snakemake.input[\"costs\"], index_col=[\"technology\", \"parameter\"])\n",
    "\n",
    "    if len(ships.index) != 1 and n.name != \"LOHC shipping\":\n",
    "        logger.warning(\n",
    "            \"More than one entry in ships.csv only supported for LOHC shipping.\"\n",
    "        )\n",
    "\n",
    "    ship = ships.iloc[0]\n",
    "\n",
    "    # Which fuel to use for ship propulsion:\n",
    "    # Either 'cargo' (default) or name of a shipping fuel in the cost csv file\n",
    "    assert (\n",
    "        \"use_fuel\" in ship\n",
    "    ), \"use_fuel needs to be defined in ships.csv (either shipping fuel or 'cargo')\"\n",
    "    fuel_to_use = ship[\"use_fuel\"]\n",
    "\n",
    "    props = props.loc[ship.name]\n",
    "\n",
    "    loading_time = int(np.floor(props.loc[\"(un-) loading time\", \"value\"]))\n",
    "    unloading_time = loading_time\n",
    "    loading_rate_pu = 1.0 / loading_time\n",
    "    unloading_rate_pu = loading_rate_pu\n",
    "\n",
    "    distance = distances.query(\n",
    "        f\"\"\"region_a == '{snakemake.wildcards['from']}' and \"\"\"\n",
    "        f\"\"\"region_b == '{snakemake.wildcards['to']}' and \"\"\"\n",
    "        f\"\"\"type == 'sea route'\"\"\",\n",
    "        engine=\"python\",\n",
    "    )[\"value\"].item()\n",
    "\n",
    "    travel_time = int(np.ceil(distance / props.loc[\"average speed\", \"value\"]))\n",
    "\n",
    "    # Round trip time for a convoy (loading, travel, unloading, return trip)\n",
    "    round_trip_time = loading_time + travel_time + unloading_time + travel_time\n",
    "\n",
    "    # Number of full journeys (round-trip-journey) possible for convoy along sea route\n",
    "    journeys = int(np.floor(n.snapshots.shape[0] / round_trip_time))\n",
    "\n",
    "    # By constructing the tightest shipping schedule starting at the beginning of the year\n",
    "    # we have this amount of hours were the importing habour is not served...\n",
    "    annual_shipping_gap = n.snapshots.shape[0] % round_trip_time\n",
    "\n",
    "    # ... as we later construct additional shipping convoys by simply shifting the schedule,\n",
    "    # this will create a nasty gap in the supply chain, resulting in weired results in the optimisation.\n",
    "    # We avoid this by smoothing the supply: the shipping duration is artificially prolonged to reduce this gap\n",
    "    # Can be thought of as something like a buffer, which is near identically distributed across all journeys\n",
    "    additional_forward_travel_time = int(np.floor(annual_shipping_gap / journeys / 2))\n",
    "    # return trip can take a bit longer (max 1 additional snapshot)\n",
    "    additional_return_travel_time = int(\n",
    "        np.floor(annual_shipping_gap / journeys - additional_forward_travel_time)\n",
    "    )\n",
    "\n",
    "    forward_travel_time = travel_time + additional_forward_travel_time\n",
    "    return_travel_time = travel_time + additional_return_travel_time\n",
    "\n",
    "    updated_round_trip_time = (\n",
    "        loading_time + forward_travel_time + unloading_time + return_travel_time\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Increasing the round-trip travel time from {round_trip_time}h to \"\n",
    "        f\"{updated_round_trip_time}h (+{(updated_round_trip_time/round_trip_time-1)*100:.2f}%) \"\n",
    "        f\"to achieve more levelled supply by ship.\"\n",
    "    )\n",
    "\n",
    "    # One round-trip loading schedule for earliest convoy in year\n",
    "    loading_schedule = np.concatenate(\n",
    "        (\n",
    "            [loading_rate_pu] * loading_time,\n",
    "            [0] * forward_travel_time,\n",
    "            [0] * unloading_time,\n",
    "            [0] * return_travel_time,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # One round-trip unloading schedule for earliest convoy in year\n",
    "    unloading_schedule = np.concatenate(\n",
    "        (\n",
    "            [0] * loading_time,\n",
    "            [0] * forward_travel_time,\n",
    "            [unloading_rate_pu] * unloading_time,\n",
    "            [0] * return_travel_time,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Numbers of convoys (base convoy + convoys which can be loaded without competing for the\n",
    "    #  loading infrastructure while the base convoy is on its way)\n",
    "    # if loading_time == unloading_time there this approach results in no clashes for the unloading infrastruct.\n",
    "    convoy_number = 1 + int(\n",
    "        np.floor(\n",
    "            (forward_travel_time + return_travel_time + unloading_time) / loading_time\n",
    "        )\n",
    "    )\n",
    "\n",
    "    shift = (forward_travel_time + unloading_time + return_travel_time) % unloading_time\n",
    "    shift_per_convoy = int(np.floor(shift / convoy_number))\n",
    "\n",
    "    # Create full year schedule for loading:\n",
    "    # Left-over days at end of year (which can not be used for a full round-trip journey)\n",
    "    # are filled with 0s (=no journey/anchored)\n",
    "    loading_schedule = np.concatenate([loading_schedule] * journeys)\n",
    "    tmp = np.zeros(n.snapshots.shape[0])\n",
    "    tmp[: loading_schedule.shape[0]] = loading_schedule\n",
    "    loading_schedule = tmp\n",
    "\n",
    "    # Create full year schedule for unloading:\n",
    "    # (Basically the same as for loading, could use np.roll here as rates and durations for loading\n",
    "    #  and unloading are identical in the current model version)\n",
    "    # Left-over days at end of year (which can not be used for a full round-trip journey)\n",
    "    # are filled with 0s (=no journey/anchored)\n",
    "    unloading_schedule = np.concatenate([unloading_schedule] * journeys)\n",
    "    tmp = np.zeros(n.snapshots.shape[0])\n",
    "    tmp[: unloading_schedule.shape[0]] = unloading_schedule\n",
    "    unloading_schedule = tmp\n",
    "\n",
    "    ## How the schedules look like\n",
    "    # plt.plot(loading_schedule, label='loading')\n",
    "    # plt.plot(unloading_schedule, label='unloading')\n",
    "    # plt.legend()\n",
    "\n",
    "    # Calculate energy transport efficiency for the trip\n",
    "\n",
    "    # Boil-off losses are only considered for the outward journey.\n",
    "    # technically the cargo hold should be kept at cryo temperatures and uncontaminated also during\n",
    "    # the inward journey. Neglect boil-off during inward journey here, as this can be expected to be\n",
    "    # significantly smaller than the onboard energy demand (all boil-off is certainly consumed by energy demand)\n",
    "    boil_off = (1 - props.loc[\"boil-off\", \"value\"] / 100) ** forward_travel_time\n",
    "\n",
    "    # Energy demand in MWh for ship propulsion (outward and return journey)\n",
    "    energy_demand = (\n",
    "        2\n",
    "        * distance\n",
    "        * props.loc[\"energy demand\", \"value\"]\n",
    "        / props.loc[\"capacity\", \"value\"]\n",
    "    )\n",
    "\n",
    "    if fuel_to_use == \"cargo\":\n",
    "        # take whatever requires more energy (boil-off can be used by propulsion or propulsion uses cargo)\n",
    "        shipping_efficiency = np.min([1 - energy_demand, boil_off])\n",
    "        assert shipping_efficiency > 0, \"Shipping (Link) efficiency must be > 0\"\n",
    "        logging.info(\"Using shipping cargo as fuel source.\")\n",
    "    else:\n",
    "        shipping_efficiency = boil_off\n",
    "\n",
    "        logging.info(\n",
    "            f\"\"\"\n",
    "            Using external shipping fuel: {fuel_to_use}\n",
    "            Adding necessary network components (fuel bus and generator for purchasing fuel).\n",
    "            \"\"\"\n",
    "        )\n",
    "        n.add(\n",
    "            \"Bus\",\n",
    "            name=f\"{fuel_to_use} bus\",\n",
    "            carrier=f\"{fuel_to_use}\",\n",
    "            unit=\"MWh\",\n",
    "        )\n",
    "\n",
    "        n.add(\n",
    "            \"Generator\",\n",
    "            name=f\"{fuel_to_use} purchase\",\n",
    "            bus=f\"{fuel_to_use} bus\",\n",
    "            carrier=f\"{fuel_to_use}\",\n",
    "            capital_cost=0.0,\n",
    "            marginal_cost=costs.loc[fuel_to_use, \"fuel\"][\"value\"],\n",
    "            p_nom_extendable=True,\n",
    "            unit=\"MWh\",\n",
    "        )\n",
    "\n",
    "    # Additional energy losses from (un-) loading the cargo\n",
    "    loading_efficiency = 1 - props.loc[\"(un-) loading losses\", \"value\"] / 100\n",
    "    unloading_efficiency = loading_efficiency\n",
    "\n",
    "    # Calculate investment costs per gross MWh capacity\n",
    "    costs = costs.loc[ship.name]\n",
    "\n",
    "    # Consistency check: whether units match\n",
    "    unit_costs = costs.loc[\"capacity\"][\"unit\"]\n",
    "    unit_bus = network.buses.loc[ship[\"bus0\"]][\"unit\"]\n",
    "    if _do_units_match(unit_bus, unit_costs) is False:\n",
    "        raise ValueError(\n",
    "            f\"Unit mismatch for {ship.name} between network ({unit_bus}) \"\n",
    "            f\"and cost database ({unit_costs}).\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        capital_cost = calculate_annuity(\n",
    "            costs.loc[\"investment\", \"value\"],\n",
    "            costs.loc[\"FOM\", \"value\"],\n",
    "            costs.loc[\"lifetime\", \"value\"],\n",
    "            wacc,\n",
    "        )\n",
    "        capital_cost /= costs.loc[\"capacity\", \"value\"]\n",
    "    except:\n",
    "        raise ValueError(\n",
    "            f\"Exception calculating capital cost for {ship.name}.\"\n",
    "            f\"Missing cost or shipping property entries\"\n",
    "        )\n",
    "\n",
    "    if distance == 0:\n",
    "        # Treat the special case, where distance between exporter and importer is zero.\n",
    "        # (e.g. same exporter as importer region)\n",
    "\n",
    "        logger.info(\n",
    "            f\"No distance between exporter and importer. \"\n",
    "            f\"Adding a direct pseudo connection without shipping schedule.\"\n",
    "        )\n",
    "\n",
    "        convoy_number = 1\n",
    "\n",
    "        loading_schedule = np.ones(n.snapshots.shape[0])\n",
    "        unloading_schedule = loading_schedule\n",
    "\n",
    "    else:\n",
    "        logger.info(f\"Adding {convoy_number} shipping convoys to shipping route.\")\n",
    "\n",
    "    for i in range(convoy_number):\n",
    "\n",
    "        ship_bus = f\"{ship.name} convoy {i+1}\"\n",
    "\n",
    "        n.add(\n",
    "            \"Bus\",\n",
    "            name=f\"{ship_bus} (exp)\",\n",
    "            carrier=network.buses.loc[ship.loc[\"bus0\"], \"carrier\"],\n",
    "            unit=network.buses.loc[ship.loc[\"bus0\"], \"unit\"],\n",
    "        )\n",
    "\n",
    "        n.add(\n",
    "            \"Link\",\n",
    "            name=f\"{ship_bus} loading\",\n",
    "            bus0=ship.loc[\"bus0\"],\n",
    "            bus1=f\"{ship_bus} (exp)\",\n",
    "            efficiency=loading_efficiency,\n",
    "            # Capacity expansion at point of export/depature\n",
    "            # ship capacity taken into account as gross capacity before transport losses/demand\n",
    "            p_nom_extendable=True,\n",
    "            # Loading extracts energy from bus and happens at max rate and at fixed times\n",
    "            # Rolling the schedules ensures there is no overlap between convoys\n",
    "            p_min_pu=np.roll(\n",
    "                loading_schedule, i * (loading_time + shift_per_convoy)\n",
    "            ),  # np.zeros_like(loading_schedule),\n",
    "            p_max_pu=np.roll(loading_schedule, i * (loading_time + shift_per_convoy)),\n",
    "        )\n",
    "\n",
    "        n.add(\n",
    "            \"Store\",\n",
    "            name=f\"{ship_bus} cargo (exp)\",\n",
    "            bus=f\"{ship_bus} (exp)\",\n",
    "            e_nom_extendable=True,\n",
    "            # Ships may starting at end of year and start deliver at the beginning of the year\n",
    "            e_cyclic=True,\n",
    "            # Full capital cost of the ship\n",
    "            capital_cost=capital_cost,\n",
    "        )\n",
    "\n",
    "        n.add(\n",
    "            \"Bus\",\n",
    "            name=f\"{ship_bus} (imp)\",\n",
    "            carrier=network.buses.loc[ship.loc[\"bus0\"], \"carrier\"],\n",
    "            unit=network.buses.loc[ship.loc[\"bus0\"], \"unit\"],\n",
    "        )\n",
    "\n",
    "        n.add(\n",
    "            \"Link\",\n",
    "            name=f\"{ship_bus} unloading\",\n",
    "            bus0=f\"{ship_bus} (imp)\",\n",
    "            bus1=ship.loc[\"bus1\"],\n",
    "            efficiency=unloading_efficiency,\n",
    "            p_nom_extendable=True,\n",
    "            # Unloading at max rate and at fixed times\n",
    "            # Rolling the schedules ensures there is no overlap between convoys\n",
    "            p_min_pu=np.roll(\n",
    "                unloading_schedule, i * (unloading_time + shift_per_convoy)\n",
    "            ),  # np.zeros_like(unloading_schedule),\n",
    "            p_max_pu=np.roll(\n",
    "                unloading_schedule, i * (unloading_time + shift_per_convoy)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        n.add(\n",
    "            \"Link\",\n",
    "            name=f\"{ship_bus} trip demand & losses\",\n",
    "            bus0=f\"{ship_bus} (exp)\",\n",
    "            bus1=f\"{ship_bus} (imp)\",\n",
    "            efficiency=shipping_efficiency,\n",
    "            p_nom_extendable=True,\n",
    "            # If not cargo but external fuel is used: Connect to fuel supplying bus\n",
    "            bus2=f\"{fuel_to_use} bus\" if fuel_to_use != \"cargo\" else \"\",\n",
    "            efficiency2=(-1) * energy_demand if fuel_to_use != \"cargo\" else np.nan,\n",
    "            # efficiency2=(-1)*energy_demand/shipping_efficiency if fuel_to_use != \"cargo\" else \"\",\n",
    "        )\n",
    "\n",
    "        # Special case LOHC:\n",
    "        # requires a return channel for unloaded LOHC using the same ship and thus a dedicated store component\n",
    "        # Warning: Partially hardcoded!\n",
    "        # Note: Relies on information from ships.csv\n",
    "        # Note: Linked to a extra_functionality in solve_network.py.ipynb fixing the two store's capacities\n",
    "        if n.name == \"LOHC shipping\":\n",
    "\n",
    "            if fuel_to_use != \"cargo\":\n",
    "                raise NotImplementedError(\n",
    "                    \"Use of external fuel (instead of cargo) is not yet implemented for LOHC shipping. Efficiency of LOHC consuming ship-internal links need to be adjusted.\"\n",
    "                )\n",
    "\n",
    "            n.add(\n",
    "                \"Bus\",\n",
    "                name=f\"{ship_bus} LOHC (used)\",\n",
    "                carrier=n.buses.loc[\"LOHC (used) (exp)\", \"carrier\"],\n",
    "                unit=n.buses.loc[\"LOHC (used) (exp)\", \"unit\"],\n",
    "            )\n",
    "\n",
    "            n.add(\n",
    "                \"Store\",\n",
    "                name=f\"{ship_bus} cargo LOHC (used)\",\n",
    "                bus=f\"{ship_bus} LOHC (used)\",\n",
    "                e_nom_extendable=True,\n",
    "                e_cyclic=True,\n",
    "                # Capital costs already considered by the cargo store for loaded LOHC\n",
    "                capital_cost=0,\n",
    "            )\n",
    "\n",
    "            n.add(\n",
    "                \"Link\",\n",
    "                name=f\"{ship_bus} LOHC (used) loading\",\n",
    "                bus0=ships.loc[\"LOHC (used) transport ship\", \"bus0\"],\n",
    "                bus1=f\"{ship_bus} LOHC (used)\",\n",
    "                efficiency=1.0,\n",
    "                p_nom_extendable=True,\n",
    "                # Loading extracts energy from bus and happens at max rate and at fixed times\n",
    "                # Rolling the schedules ensures there is no overlap between convoys\n",
    "                p_min_pu=np.roll(\n",
    "                    unloading_schedule, i * (unloading_time + shift_per_convoy)\n",
    "                ),  # np.zeros_like(unloading_schedule),\n",
    "                p_max_pu=np.roll(\n",
    "                    unloading_schedule, i * (unloading_time + shift_per_convoy)\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            n.add(\n",
    "                \"Link\",\n",
    "                name=f\"{ship_bus} LOHC (used) unloading\",\n",
    "                bus0=f\"{ship_bus} LOHC (used)\",\n",
    "                bus1=ships.loc[\"LOHC (used) transport ship\", \"bus1\"],\n",
    "                efficiency=1.0,\n",
    "                p_nom_extendable=True,\n",
    "                # Unloading at max rate and at fixed times\n",
    "                # Rolling the schedules ensures there is no overlap between convoys\n",
    "                p_min_pu=np.roll(\n",
    "                    loading_schedule, i * (loading_time + shift_per_convoy)\n",
    "                ),  # np.zeros_like(loading_schedule),\n",
    "                p_max_pu=np.roll(\n",
    "                    loading_schedule, i * (loading_time + shift_per_convoy)\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Propulsion energy demand\n",
    "            # if LOHc cargo is to be used for propulsion, make the link a multi-link\n",
    "            # such that unloaded LOHC is accounted for\n",
    "            if fuel_to_use == \"cargo\":\n",
    "                # Ratio LOHC / H2 in loaded LOHC\n",
    "                loaded_unloaded_ratio = read_efficiencies(\n",
    "                    snakemake.input[\"efficiencies\"], snakemake.wildcards[\"year\"]\n",
    "                )\n",
    "                loaded_unloaded_ratio = loaded_unloaded_ratio.loc[\n",
    "                    (loaded_unloaded_ratio[\"process\"] == \"LOHC dehydrogenation\")\n",
    "                    & (loaded_unloaded_ratio[\"from\"] == \"LOHC (loaded)\")\n",
    "                    & (loaded_unloaded_ratio[\"to\"] == \"LOHC (used)\")\n",
    "                ][\"efficiency\"].item()\n",
    "                n.links.loc[\n",
    "                    f\"{ship_bus} trip demand & losses\", \"bus2\"\n",
    "                ] = f\"{ship_bus} LOHC (used)\"\n",
    "                n.links.loc[f\"{ship_bus} trip demand & losses\", \"efficiency2\"] = (\n",
    "                    1 - shipping_efficiency\n",
    "                ) * loaded_unloaded_ratio\n",
    "    \n",
    "    # Add meta information on shipping to the network\n",
    "    # Used in solve_network for optional simplification of shipping\n",
    "    n.meta[\"shipping\"] = {\"ship_name\":ship.name, \"convoy_number\":convoy_number, \"loading_time\":loading_time, \"unloading_time\":unloading_time}\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ed328",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    scenario = snakemake.params[\"scenario\"]\n",
    "\n",
    "    configure_logging(snakemake)\n",
    "\n",
    "    # DO NOT CHANGE THIS ORDER; dependencies between methods not explicit\n",
    "    network = create_network()\n",
    "    network = attach_efficiencies(network)\n",
    "    network = attach_costs(network)\n",
    "\n",
    "    network = scale_transportation_with_distance(network)\n",
    "\n",
    "    network = add_shipping(network)\n",
    "\n",
    "    network = override_costs_for_special_cases(network)\n",
    "\n",
    "    network.export_to_netcdf(snakemake.output[\"network\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
