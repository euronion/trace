{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypsa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from _helpers import calculate_annual_investment, calculate_annuity\n",
    "from _helpers import extract_technology, extract_unit\n",
    "\n",
    "from _helpers import configure_logging\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_network():\n",
    "    \"\"\"Create the pypsa network scaffolding for the ESC.\"\"\"\n",
    "    \n",
    "    # Modify PyPSA 'Link' component to allow for 2 output busses by overwriting component_attrs\n",
    "    # c.f. https://www.pypsa.org/examples/chp-fixed-heat-power-ratio.html\n",
    "    \n",
    "    # Load additional components\n",
    "    with open(snakemake.input[\"additional_components\"], \"rb\") as f:\n",
    "        override_component_attrs = pickle.load(f)\n",
    "\n",
    "    # Create network with modified link-component\n",
    "    network = pypsa.Network(override_component_attrs=override_component_attrs)\n",
    "\n",
    "    # Load network components from csv files\n",
    "    network.import_from_csv_folder(snakemake.input[\"network\"])\n",
    "    \n",
    "    # Equally weighted snapshots, year defined via config\n",
    "    year = snakemake.config[\"scenario\"][\"year\"]\n",
    "    # Handle leap year by dropping 29th of Februray\n",
    "    snapshots = pd.date_range(str(year),str(year+1), freq=\"H\", closed=\"left\")\n",
    "    snapshots = filter(lambda x: not((x.month==2) & (x.day==29)), list(snapshots))\n",
    "    snapshots = [x.strftime('%Y-%m-%d %H:%M:%S') for x in snapshots]\n",
    "    network.set_snapshots(snapshots=snapshots)\n",
    "    \n",
    "    return network\n",
    "\n",
    "def attach_efficiencies(network):\n",
    "    \"\"\"Attach dedicated efficiencies from file.\n",
    "\n",
    "    The efficiencies are from an additional csv file and added to the links in the pypsa network\n",
    "    Format for efficiencies.csv file:\n",
    "    * \"from\" and \"to\" must substrings of the bus names\n",
    "    * \"process\" must be a substring of the name of the link\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    network : pypsa.network\n",
    "        network with external efficiencies attached to all links.\n",
    "\n",
    "    \"\"\"\n",
    "    efficiencies = pd.read_csv(snakemake.input[\"efficiencies\"])\n",
    "\n",
    "    def get_efficiency(tech, src_bus, tar_bus):\n",
    "\n",
    "        src = extract_technology(src_bus)\n",
    "        tar = extract_technology(tar_bus)\n",
    "\n",
    "        efficiency = efficiencies[(efficiencies['process'] == tech) &\n",
    "                                 (efficiencies['to'] == tar) &\n",
    "                                 (efficiencies['from'] == src)]\n",
    "    \n",
    "        if efficiency.empty is True:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return efficiency['efficiency'].item()\n",
    "\n",
    "    links = network.links\n",
    "\n",
    "    for idx, row in links.iterrows():\n",
    "\n",
    "        lead_efficiency = get_efficiency(extract_technology(row.name), row['bus0'], row['bus1'])\n",
    "        links.loc[idx, 'efficiency'] = lead_efficiency\n",
    "\n",
    "        additional_buses = {c for c in links.columns if c.startswith('bus') and row[c] != \"\"}-{'bus0','bus1'}\n",
    "        for b in additional_buses:\n",
    "\n",
    "            # by design decision all buses busn (n>1, e.g. bus2, bus3, ...) either:\n",
    "            # case 1. contribute to the output to bus1, e.g. bus2 feeds into bus1\n",
    "            # or\n",
    "            # case 2. are few by bus0.\n",
    "            # Try to retrieve both efficiencies: One should always be nan, the other one is taken.\n",
    "            # If either one are nan or not nan, throw an error.\n",
    "            # \n",
    "            # Case 1.:\n",
    "            # Efficiencies are provided for the conversion from bus2 to bus1\n",
    "            # and are thus weighted by the primary efficiency of bus1\n",
    "            # Efficiencies are have to become negative to correctly account for the flow.\n",
    "            follow_efficiency = (-1) * lead_efficiency / get_efficiency(extract_technology(row.name), row[b], row['bus1'])\n",
    "\n",
    "            # Case 2.:\n",
    "            # Efficiencies are provided for conversion from bus0 to busn.\n",
    "            # This type of efficiency does not need to be adjusted.\n",
    "            regular_efficiency = efficiency = get_efficiency(extract_technology(row.name), row['bus0'], row[b])\n",
    "            \n",
    "            e = np.array([regular_efficiency, follow_efficiency])\n",
    "            \n",
    "            if np.isnan(e).all():\n",
    "                logger.error(f\"No efficiency found for link {row.name} between \"\n",
    "                               f\"{row[b]} <-> ({row['bus0']} or {row['bus1']}).\")\n",
    "            elif np.isnan(e).sum() == 0:\n",
    "                logger.error(f\"Two efficiencies found for link {row.name} between \"\n",
    "                             f\"{row[b]} <-> ({row['bus0']} and {row['bus1']}). \"\n",
    "                             f\"Efficiency must by unambigious.\")\n",
    "            else:\n",
    "                links.loc[idx, 'efficiency'+b.replace('bus','')] = e[~np.isnan(e)][0]\n",
    "\n",
    "    return network\n",
    "\n",
    "def override_costs_for_special_cases(n):\n",
    "\n",
    "    # battery inverter represented by two links (charging and discharging),\n",
    "    # while costs in cost data are for bidirectional inverter --> correction here\n",
    "    links = n.links\n",
    "    idx = links.query('name.str.startswith(\"battery inverter\")', engine='python')['capital_cost'].index\n",
    "    links.loc[idx, 'capital_cost'] /= 2.\n",
    "    \n",
    "    # LOHC chemical enters the model through generators starting with \"LOHC chemical\"\n",
    "    # The cost for these generators is not determined by their production capacitiy,\n",
    "    # but by their production per t of LOHC chemical.\n",
    "    # Correct this here.\n",
    "    idx = n.generators.query(\"index.str.startswith('LOHC chemical')\", engine=\"python\").index\n",
    "    n.generators.loc[idx, 'marginal_cost'] = n.generators.loc[idx, 'capital_cost']\n",
    "    n.generators.loc[idx, 'capital_cost'] = 0.\n",
    "    \n",
    "    # Stores in LOHC case cost money for buying the LOHC\n",
    "    if 'LOHC' in network.name :\n",
    "        marginal_cost = n.generators.loc['LOHC chemical (exp)', 'marginal_cost']\n",
    "\n",
    "        # Raw costs per t of unloaded LOHC\n",
    "        idx = n.stores.filter(like='LOHC (unloaded) tank', axis=0).index\n",
    "        n.stores.loc[idx, 'capital_cost'] = marginal_cost\n",
    "\n",
    "        # Modified costs per t of loaded LOHC, assuming 5.6 wt-% H2 per t of LOHC\n",
    "        # WARNING: Hardcoded! Valid for H0-DBT/H18-DBT with effective discharge ratio.\n",
    "        idx = n.stores.filter(like='LOHC (loaded) tank', axis=0).index\n",
    "        n.stores.loc[idx, 'capital_cost'] = marginal_cost*(1-5.6/100.)\n",
    "\n",
    "    return n\n",
    "\n",
    "def attach_costs(network):\n",
    "    \"\"\"\n",
    "    Attach the overnight investment costs (capital costs) to the network.\n",
    "    \n",
    "    Costs are calculated from investment costs and FOM using EAC method\n",
    "    and wacc as specified via config/snakemake.input files.\n",
    "    Components name need to follow the scheme '<name> (<exp|imp>)'\n",
    "    where '<name>' must correspond to the component in the costs.csv file.    \n",
    "    \"\"\"\n",
    "    wacc = pd.read_csv(snakemake.input['wacc'], comment='#', index_col=\"region\")\n",
    "    wacc = wacc.loc[snakemake.wildcards[\"from\"], snakemake.config[\"scenario\"][\"wacc\"]]\n",
    "    \n",
    "    costs = pd.read_csv(snakemake.input['costs'], index_col=['technology','parameter'])  \n",
    "        \n",
    "    def attach_component_costs(network, component):    \n",
    "\n",
    "        components = getattr(network, component)\n",
    "        for idx, row in components.iterrows():\n",
    "\n",
    "            try:\n",
    "                tech = costs.loc[extract_technology(idx)]\n",
    "            except KeyError:\n",
    "                logger.info(f\"No cost assumptions found for {idx}.\")\n",
    "                continue\n",
    "\n",
    "            # Compare units between bus and cost data; scale investment on-demand\n",
    "            investment_unit = tech.loc['investment']['unit'].replace(\"EUR/\",\"\").replace(\"(\",\"\").replace(\")\",\"\")\n",
    "\n",
    "            investment_factor = 1.\n",
    "            \n",
    "            # Determine unit of the bus - depends on component type\n",
    "            bus_unit = bus0_unit = bus1_unit = None\n",
    "            if component=='stores':\n",
    "                bus_unit = network.buses.loc[row['bus']]['unit']\n",
    "            elif component=='generators':\n",
    "                bus_unit = network.buses.loc[row['bus']]['unit']\n",
    "            elif component=='links':\n",
    "                # Check bus0 and bus1 (compulsory for all links) for matching units \n",
    "                bus0_unit = network.buses.loc[row['bus0']]['unit']\n",
    "                bus1_unit = network.buses.loc[row['bus1']]['unit']\n",
    "                \n",
    "                \n",
    "                if investment_unit.startswith(bus0_unit): # Full match\n",
    "                    bus_unit = bus0_unit\n",
    "                elif investment_unit.startswith(bus1_unit): # Full match\n",
    "                    bus_unit = bus1_unit\n",
    "                    # bus1 is output by convention; cost in output units, i.e. scale to input units\n",
    "                    investment_factor *= row['efficiency']\n",
    "                elif investment_unit[1:].startswith(bus0_unit[1:]): # Partial match modulo prefix\n",
    "                    bus_unit = bus0_unit\n",
    "                elif investment_unit[1:].startswith(bus1_unit[1:]): # Partial match modulo prefix\n",
    "                    bus_unit = bus1_unit\n",
    "                    # bus1 is output by convention; cost in output units, i.e. scale to input units\n",
    "                    investment_factor *= row['efficiency']\n",
    "                    \n",
    "            # Consistency check: Correct units\n",
    "            # First condition relevant if prefixed does not match (try to recover below)\n",
    "            # Warning: Does not catch case were bus unit is only a single letter\n",
    "            if investment_unit[1:].startswith(bus_unit[1:]) is False:\n",
    "                raise ValueError(f'Could not find matching cost data for {component} \"{idx}\": '\n",
    "                                 f'Expected one of [{bus_unit}, {bus0_unit}, {bus1_unit}] from network, '\n",
    "                                 f'but found {investment_unit} in costs.')\n",
    "            \n",
    "            prefix_bus_unit = bus_unit[0]\n",
    "            prefix_investment_unit = investment_unit[0]\n",
    "            \n",
    "            if prefix_bus_unit == prefix_investment_unit:\n",
    "                investment_factor *= 1.\n",
    "            elif prefix_bus_unit == \"M\" and prefix_investment_unit == \"k\":\n",
    "                investment_factor *= 1.e3\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot scale between {prefix_bus_unit} and {prefix_investment_unit} \"\n",
    "                                 f\"for {idx} costs.\")\n",
    "\n",
    "            # Some technologies are without FOM values\n",
    "            # (e.g. battery capacity where FOM is attributed to the link/inverter/charger capacities)\n",
    "            try:\n",
    "                fom = tech.loc['FOM','value']\n",
    "            except KeyError:\n",
    "                logger.info(f\"No FOM for {idx}, assuming 0%.\")\n",
    "                fom = 0.\n",
    "\n",
    "            capital_cost = calculate_annuity(tech.loc['investment','value']*investment_factor,\n",
    "                                             fom, tech.loc['lifetime','value'], wacc)\n",
    "            components.loc[idx, 'capital_cost'] = capital_cost\n",
    "        \n",
    "        return network\n",
    "\n",
    "    network = attach_component_costs(network, 'links')\n",
    "    network = attach_component_costs(network, 'stores')\n",
    "    network = attach_component_costs(network, 'generators')\n",
    "\n",
    "    network = override_costs_for_special_cases(network)\n",
    "    \n",
    "    return network\n",
    "\n",
    "def scale_transportation_with_distance(n, link_types=['HVDC overhead','pipeline']):\n",
    "    \"\"\"Scale transportation of chemical energy carriers by distance.\n",
    "    \n",
    "    Changes costs and efficiencies for links related to the link_types based on the distance\n",
    "    between exporter and importer. Does not touch the efficiencies or costs or shipping\n",
    "    (they are treated separately).\n",
    "    \"\"\"\n",
    "    \n",
    "    links = n.links\n",
    "    distances = pd.read_csv(snakemake.input['distances'], comment='#', quotechar='\"')\n",
    "\n",
    "    mapping = {\n",
    "        'HVDC overhead':\n",
    "            {\n",
    "                'distance_type': 'as-the-crow-flies',\n",
    "                'detour_factor_key': 'transmission_line'\n",
    "            },\n",
    "        'pipeline':\n",
    "            {\n",
    "                'distance_type': 'as-the-crow-flies',\n",
    "                'detour_factor_key': 'pipeline'\n",
    "            }\n",
    "    }\n",
    "\n",
    "    distances = distances[(distances['region_a']==snakemake.wildcards['from']) &\n",
    "                          (distances['region_b']==snakemake.wildcards['to'])].set_index('type')\n",
    "\n",
    "    efs = [c for c in links.columns if c.startswith('efficiency')]\n",
    "\n",
    "    for link_type in link_types:\n",
    "\n",
    "        m = mapping[link_type]\n",
    "        detour_factor = snakemake.config['detour_factors'][m['detour_factor_key']]\n",
    "        distance = distances.loc[m['distance_type'], 'value']\n",
    "\n",
    "        for idx, row in links.query('name.str.contains(@link_type)', engine='python').iterrows():\n",
    "            links.loc[idx, 'length'] = distance\n",
    "            links.loc[idx, 'capital_cost'] *= distance # capital cost in EUR/km\n",
    "\n",
    "            links.loc[idx, efs] = np.sign(row[efs].astype(np.float))*(np.abs(row[efs])**(distance/1.e3)) # efficiencies in p.u./1000km; avoid imaginary numbers for negative efficiencies as base (>=2 input buses for links)\n",
    "\n",
    "    return n\n",
    "\n",
    "def add_shipping(n):\n",
    "    \"\"\"Adds optional shipping routes to the network.\n",
    "    \n",
    "    Checks whether file \"<ESC>/ships.csv\" exists and - if it does - constructs\n",
    "    a shipping route with multiple convoys (as optimisation options) for this route\n",
    "    using standard PyPSA components.\n",
    "    \n",
    "    Limitation: ONLY SUPPORTS ONE SHIPPING CONNECTION PER NETWORK AT THE MOMENT!\n",
    "    \"\"\"\n",
    "    \n",
    "    fn = Path(snakemake.input['network'])/\"ships.csv\"\n",
    "\n",
    "    # Network without shipping routes\n",
    "    if not fn.exists():\n",
    "        return n\n",
    "    else:\n",
    "        ships = pd.read_csv(fn, comment='#', index_col='name')\n",
    "\n",
    "    props = pd.read_csv(snakemake.input['shipping_properties'], comment='#', index_col=['name','variable'])\n",
    "    \n",
    "    distances = pd.read_csv(snakemake.input['distances'], comment='#', quotechar='\"')\n",
    "    \n",
    "    if len(ships.index) != 1:\n",
    "        raise ValueError(\"Number of shipping lanes defined in ships.csv must be exactly one.\")\n",
    "        \n",
    "    ship = ships.iloc[0]\n",
    "\n",
    "    props = props.loc[ship.name]\n",
    "\n",
    "    loading_time = np.int(np.floor(props.loc['(un-) loading time', 'value']))\n",
    "    unloading_time = loading_time\n",
    "    loading_rate_pu = 1./loading_time\n",
    "    unloading_rate_pu = loading_rate_pu\n",
    "\n",
    "    distance = distances.query(f\"\"\"region_a == '{snakemake.wildcards['from']}' and \"\"\"\n",
    "                    f\"\"\"region_b == '{snakemake.wildcards['to']}' and \"\"\"\n",
    "                    f\"\"\"type == 'sea route'\"\"\", engine='python')['value'].item()\n",
    "\n",
    "    travel_time = np.int(np.ceil(distance / props.loc['average speed','value']))\n",
    "\n",
    "    # Round trip time for a convoy (loading, travel, unloading, return trip)\n",
    "    round_trip_time = loading_time + travel_time + unloading_time + travel_time\n",
    "\n",
    "    # Number of full journeys (round-trip-journey) possible for convoy along sea route\n",
    "    journeys = np.int(np.floor(n.snapshots.shape[0]/round_trip_time))\n",
    "\n",
    "    # By constructing the tightest shipping schedule starting at the beginning of the year\n",
    "    # we have this amount of hours were the importing habour is not served...\n",
    "    annual_shipping_gap = n.snapshots.shape[0]%round_trip_time\n",
    "\n",
    "    # ... as we later construct additional shipping convoys by simply shifting the schedule,\n",
    "    # this will create a nasty gap in the supply chain, resulting in weired results in the optimisation.\n",
    "    # We avoid this by smoothing the supply: the shipping duration is artificially prolonged to reduce this gap\n",
    "    # Can be thought of as something like a buffer, which is near identically distributed across all journeys\n",
    "    additional_forward_travel_time = np.int(np.floor(annual_shipping_gap / journeys / 2))\n",
    "    # return trip can take a bit longer (max 1 additional snapshot)\n",
    "    additional_return_travel_time = np.int(np.floor(annual_shipping_gap / journeys - additional_forward_travel_time))\n",
    "\n",
    "    forward_travel_time = travel_time+additional_forward_travel_time\n",
    "    return_travel_time = travel_time+additional_return_travel_time\n",
    "    \n",
    "    updated_round_trip_time = loading_time + forward_travel_time + unloading_time + return_travel_time\n",
    "    logger.info(f\"Increasing the round-trip travel time from {round_trip_time}h to \"\n",
    "                f\"{updated_round_trip_time}h (+{(updated_round_trip_time/round_trip_time-1)*100:.2f}%) \"\n",
    "                f\"to achieve more levelled supply by ship.\")\n",
    "\n",
    "    # One round-trip loading schedule for earliest convoy in year\n",
    "    loading_schedule = np.concatenate(([loading_rate_pu]*loading_time,\n",
    "                                        [0]*forward_travel_time,\n",
    "                                        [0]*unloading_time,\n",
    "                                        [0]*return_travel_time))\n",
    "\n",
    "    # One round-trip unloading schedule for earliest convoy in year\n",
    "    unloading_schedule = np.concatenate(([0]*loading_time,\n",
    "                                         [0]*forward_travel_time,\n",
    "                                         [unloading_rate_pu]*unloading_time,\n",
    "                                         [0]*return_travel_time\n",
    "                                        ))\n",
    "\n",
    "    # Numbers of convoys (base convoy + convoys which can be loaded without competing for the\n",
    "    #  loading infrastructure while the base convoy is on its way)\n",
    "    # if loading_time == unloading_time there this approach results in no clashes for the unloading infrastruct.\n",
    "    convoy_number = 1 + np.int(np.floor((forward_travel_time+return_travel_time+unloading_time)/loading_time))\n",
    "\n",
    "    # Create full year schedule for loading:\n",
    "    # Left-over days at end of year (which can not be used for a full round-trip journey)\n",
    "    # are filled with 0s (=no journey/anchored)\n",
    "    loading_schedule = np.concatenate([loading_schedule]*journeys)\n",
    "    tmp = np.zeros(n.snapshots.shape[0])\n",
    "    tmp[:loading_schedule.shape[0]] = loading_schedule\n",
    "    loading_schedule = tmp\n",
    "\n",
    "    # Create full year schedule for unloading:\n",
    "    # (Basically the same as for loading, could use np.roll here as rates and durations for loading\n",
    "    #  and unloading are identical in the current model version)\n",
    "    # Left-over days at end of year (which can not be used for a full round-trip journey)\n",
    "    # are filled with 0s (=no journey/anchored)\n",
    "    unloading_schedule = np.concatenate([unloading_schedule]*journeys)\n",
    "    tmp = np.zeros(n.snapshots.shape[0])\n",
    "    tmp[:unloading_schedule.shape[0]] = unloading_schedule\n",
    "    unloading_schedule = tmp\n",
    "\n",
    "    ## How the schedules look like\n",
    "    # plt.plot(loading_schedule, label='loading')\n",
    "    # plt.plot(unloading_schedule, label='unloading')\n",
    "    # plt.legend()\n",
    "\n",
    "    # Calculate energy transport efficiency for the trip\n",
    "    # losses due to boil-off\n",
    "    boil_off = (1-props.loc['boil-off','value']/100)**forward_travel_time\n",
    "    # losses from ship propulsion (outward and return journey)\n",
    "    energy_demand = (1-2*distance*props.loc['energy demand','value']/props.loc['capacity','value'])\n",
    "    # take whatever requires more energy (boil-off can be used by propulsion or propulsion uses cargo)\n",
    "    shipping_efficiency = np.min([energy_demand, boil_off])\n",
    "    \n",
    "    # Additional energy losses from (un-) loading the cargo\n",
    "    loading_efficiency = (1-props.loc['(un-) loading losses','value']/100)\n",
    "    unloading_efficiency = loading_efficiency\n",
    "\n",
    "    # Calculate investment costs per gross MWh capacity\n",
    "    wacc = pd.read_csv(snakemake.input['wacc'], comment='#', index_col=\"region\")\n",
    "    wacc = wacc.loc[snakemake.wildcards[\"from\"], snakemake.config[\"scenario\"][\"wacc\"]]\n",
    "    \n",
    "    costs = pd.read_csv(snakemake.input['costs'], index_col=['technology','parameter'])\n",
    "    costs = costs.loc[ship.name]\n",
    "\n",
    "    # Consistency check: whether units match\n",
    "    unit_costs = costs.loc['capacity']['unit']\n",
    "    unit_bus = network.buses.loc[ship['bus0']]['unit']\n",
    "    if unit_costs.startswith(unit_bus) is False:\n",
    "        raise ValueError(f\"Unit mismatch for shipping capacity between network ({unit_bus}) \"\n",
    "                         f\"and cost database ({unit_costs}).\")\n",
    "    \n",
    "    try:\n",
    "        capital_cost = calculate_annuity(costs.loc['investment','value'],\n",
    "                                     costs.loc['FOM','value'],\n",
    "                                     costs.loc['lifetime','value'],\n",
    "                                     wacc)\n",
    "        capital_cost /= costs.loc['capacity','value']\n",
    "    except:\n",
    "        raise ValueError(f\"Exception calculating capital cost for {ship.name}.\"\n",
    "                         f\"Missing cost or shipping property entries\")\n",
    "        \n",
    "    if distance == 0:\n",
    "        # Treat the special case, where distance between exporter and importer is zero.\n",
    "        # (e.g. same exporter as importer region)\n",
    "    \n",
    "        logger.info(f\"No distance between exporter and importer. \"\n",
    "                    f\"Adding a direct pseudo connection without shipping schedule.\")\n",
    "        \n",
    "        convoy_number = 1\n",
    "        \n",
    "        loading_schedule = np.ones(n.snapshots.shape[0])\n",
    "        unloading_schedule = loading_schedule\n",
    "        \n",
    "    else:\n",
    "        logger.info(f\"Adding {convoy_number} shipping convoys to shipping route.\")        \n",
    "        \n",
    "    for i in range(convoy_number):\n",
    "\n",
    "        ship_bus = f\"{ship.name} convoy {i+1}\"\n",
    "\n",
    "        n.add(\"Bus\",\n",
    "              name=ship_bus,\n",
    "              carrier=network.buses.loc[ship.loc['bus0'],'carrier'],\n",
    "              unit=network.buses.loc[ship.loc['bus0'],'unit'],\n",
    "             )\n",
    "\n",
    "        n.add(\"Store\",\n",
    "              name=f\"{ship_bus} cargo\",\n",
    "              bus=ship_bus,\n",
    "              e_nom_extendable=True,\n",
    "              e_cyclic=True, # Ships may starting at end of year and start deliver at the beginning of the year\n",
    "              capital_cost=capital_cost,\n",
    "             )\n",
    "\n",
    "        n.add(\"Link\",\n",
    "              name=f\"{ship_bus} loading\",\n",
    "              bus0=ship.loc['bus0'],\n",
    "              bus1=ship_bus,\n",
    "              efficiency=loading_efficiency,\n",
    "              # Capacity expansion at point of export/depature\n",
    "              # ship capacity taken into account as gross capacity before transport losses/demand\n",
    "              p_nom_extendable=True,\n",
    "              p_nom=0.,\n",
    "              capital_cost=0.,\n",
    "              # Loading extracts energy from bus and happens at max rate and at fixed times\n",
    "              # Rolling the schedules ensures there is no overlap between convoys\n",
    "              p_min_pu=np.zeros_like(loading_schedule),\n",
    "              p_max_pu=np.roll(loading_schedule,i*loading_time),\n",
    "             )\n",
    "\n",
    "        n.add(\"Link\",\n",
    "              name=f\"{ship_bus} unloading\",\n",
    "              bus0=ship_bus,\n",
    "              bus1=ship.loc['bus1'],\n",
    "              efficiency=shipping_efficiency*unloading_efficiency,\n",
    "              p_nom_extendable=True,\n",
    "              p_nom=0.,\n",
    "              # Costs for shipping are fully accounted by the \"export generator\" above\n",
    "              capital_cost=0.,\n",
    "              # Unloading at max rate and at fixed times\n",
    "              # Rolling the schedules ensures there is no overlap between convoys\n",
    "              p_min_pu=np.zeros_like(unloading_schedule),\n",
    "              p_max_pu=np.roll(unloading_schedule,i*loading_time),\n",
    "             )    \n",
    "        \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    configure_logging(snakemake)\n",
    "    \n",
    "    network = create_network()\n",
    "\n",
    "    network = attach_efficiencies(network)\n",
    "    network = attach_costs(network)\n",
    "    network = scale_transportation_with_distance(network)\n",
    "    network = add_shipping(network)\n",
    "\n",
    "    network.export_to_netcdf(snakemake.output['network'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
