{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26336c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import dateutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pypsa\n",
    "from _helpers import configure_logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    scenario = snakemake.params[\"scenario\"]\n",
    "    configure_logging(snakemake)\n",
    "\n",
    "    with open(snakemake.input[\"additional_components\"], \"rb\") as f:\n",
    "        override_component_attrs = pickle.load(f)\n",
    "\n",
    "    network = pypsa.Network(override_component_attrs=override_component_attrs)\n",
    "    network.import_from_netcdf(snakemake.input[\"network\"])\n",
    "\n",
    "    # Determine all loads related to imports (on the importer side of the ESC)\n",
    "    import_loads = network.loads[network.loads.bus.str.contains(\"imp\")]\n",
    "\n",
    "    # Read load profile from file\n",
    "    df = pd.read_csv(snakemake.input[\"import_profile\"], index_col=\"snapshot\")\n",
    "\n",
    "    # Conversion to datetime required for pd.update() later\n",
    "    # Index must match that of network.snapshots\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # File format consistency checks\n",
    "    if {\"snapshot\", \"profile [p.u.]\"} != set(\n",
    "        pd.read_csv(snakemake.input[\"import_profile\"]).columns\n",
    "    ):\n",
    "        logger.error(\n",
    "            f\"Import profile file '{snakemake.input['import_profile']}' must have \"\n",
    "            f\"exactly two columns 'snapshot' and 'profile [p.u.]'\"\n",
    "        )\n",
    "    if df.index.equals(network.snapshots) is False:\n",
    "        logger.error(\n",
    "            f\"The 'snapshot' index from '{snakemake.input['import_profile']}' \"\n",
    "            f\"does not match the network snapshots: {network.snapshots[:5], ...}. \"\n",
    "            f\"One or more wrong entries in the input file?\"\n",
    "        )\n",
    "\n",
    "    # Create import_profile with each demand attached to an import bus the network\n",
    "    import_profiles = pd.DataFrame(\n",
    "        {n: p[\"p_set\"] * df[\"profile [p.u.]\"] for n, p in import_loads.iterrows()}\n",
    "    )\n",
    "\n",
    "    # Add import profiles to network\n",
    "    network.loads_t[\"p_set\"] = network.loads_t[\"p_set\"].join(import_profiles)\n",
    "\n",
    "    # Remove static demand for these loads\n",
    "    network.loads.loc[import_profiles.columns, \"p_set\"] = 0\n",
    "\n",
    "    # Add optional buffers to the import loads\n",
    "    if scenario.get(\"import_buffer\", False) is False:\n",
    "        logger.info(\"Imports are not buffered.\")\n",
    "    else:\n",
    "\n",
    "        # TODO : not required? remove?",
    "        freqs = {\n",
    "            \"annually\": \"Y\",\n",
    "            \"quaterly\": \"Q\",\n",
    "            \"monthly\": \"M\",\n",
    "            \"weekly\": \"W\",\n",
    "            \"biweekly\": \"SM\",\n",
    "            \"daily\": \"D\",\n",
    "        }\n",
    "\n",
    "        if scenario[\"import_buffer\"] not in freqs:\n",
    "            logger.error(\n",
    "                f\"Unknown 'import_buffer' option '{scenario['import_buffer']}'.\"\n",
    "            )\n",
    "\n",
    "        # Buffer sized to match the maximum demand of any time-group based on the buffer duration,\n",
    "        # i.e. the buffer size can fully accomodate the demand within the buffer duration\n",
    "        buffer_e_nom_max = network.loads_t[\"p_set\"][import_loads.index].groupby(\n",
    "            pd.Grouper(freq=freqs[scenario[\"import_buffer\"]])\n",
    "        )\n",
    "        buffer_e_nom_max = np.round(buffer_e_nom_max.sum().max().values)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Imports are buffered {scenario['import_buffer']}, \"\n",
    "            f\"adding {buffer_e_nom_max} MWh buffer to {import_loads.index.values} .\"\n",
    "        )\n",
    "\n",
    "        # Implement import_buffer by adding a store\n",
    "        # to the bus of each load on the import side\n",
    "        # with limited maximum capacity equal to max. time-span (\"import_buffer\") import demand\n",
    "        network.madd(\n",
    "            \"Store\",\n",
    "            names=\"Buffer: \" + import_loads.index,\n",
    "            bus=import_loads.bus.values,\n",
    "            e_nom_extendable=True,\n",
    "            e_nom_max=buffer_e_nom_max,\n",
    "            e_cyclic=True,\n",
    "        )\n",
    "\n",
    "    network.export_to_netcdf(snakemake.output[\"network\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
